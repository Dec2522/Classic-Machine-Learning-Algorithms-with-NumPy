{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a supervised machine learning algorithm that is used for both classification and regression tasks. It works by splitting the dataset based on feature values. This is a common process in a humans decision making e.g. I want to buy a new pair of shoes, first I separate by brand, then from that brand the styles I like, and finally from those styles which ones are in my price range.\n",
        "\n",
        "The structure follows:\n",
        "\n",
        "* Internetal node which represents the decision rule on a feature (for each data point feature =< threshold).\n",
        "* Branch representing the outcome of that rule (left = true, right = false).\n",
        "* Leaf node or terminal node which represents the final output. For classification this is the most common class of samples in that node. For regressions this is an average of the target values in that node.\n"
      ],
      "metadata": {
        "id": "-3y8_51nvZcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explanation"
      ],
      "metadata": {
        "id": "yObyQI1uJ_K5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this structure in mind, see below the defined `Node` helper class which is used to represent a single node in the tree and it's information.    \n",
        "    \n",
        "    \n",
        "    class Node:\n",
        "        '''\n",
        "        Arguments:\n",
        "        feature -- which column the threshold will refer to\n",
        "        threshold -- the splitting parameter\n",
        "        left -- left child node\n",
        "        right -- right child node\n",
        "        value -- prediction, exist only if it is leaf node.\n",
        "        '''\n",
        "        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "            self.feature = feature\n",
        "            self.threshold = threshold\n",
        "            self.left = left\n",
        "            self.right = right\n",
        "            self.value = value"
      ],
      "metadata": {
        "id": "BJfDuUzqF2lR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before creating the Decision Tree class, I will go through each of its functions."
      ],
      "metadata": {
        "id": "ShI3fSEy0GGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Initialise the tree's parameters.\n",
        "* `min_samples_split` - the minimum number of samaples required to split and internal node. Prevents splitting without too few nodes which avoids overfitting.\n",
        "* `max_depth` - Maxmimum depth or max number of recursive splits. Prevents overfitting and controls computation cost.\n",
        "* `measure_name` - Defines what criterion to ues to evaluate the quality of a decision rule.\n",
        "* `root` - First node in the tree\n",
        "\n",
        "\n",
        "      def __init__(self, min_samples_split=2, max_depth=100, measure_name=\"entropy\", n_feats=None):\n",
        "          self.min_samples_split = min_samples_split\n",
        "          self.max_depth = max_depth\n",
        "          self.measure_name = measure_name\n",
        "          self.root = None"
      ],
      "metadata": {
        "id": "vdfy_6yF0isV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Start decision making\n",
        "\n",
        "`fit` - call to start growing the tree\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      self.root = self._grow_tree(X, y, depth=0)\n",
        "\n",
        "`_grow_tree` - recursively find best split to grow tree until leaf node.\n",
        "\n",
        "* Check depth and number of samples fulfill requirements, if not add the most common value to the Node class's value (becomes leaf node).\n",
        "* If requirements are fulfilled find the best split of the input data.\n",
        "* Based on the best split, create then left and right splits, and grow the tree again.\n",
        "* Add the details of this split to the node class.\n",
        "\n",
        "\n",
        "      def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # stopping criteria\n",
        "        if depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split:\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # greedily select the best split according to information gain\n",
        "        best_feat, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # grow the children that result from the split\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feat, best_thresh, left, right)\n",
        "\n",
        "Splits are not random, they are based on which split will reduce the uncertainty of the target variable the most.\n",
        "\n",
        "`_best_split` - loop through every feature and data point within that feature to see which combination results in the best information gain; data point being at which threshold in that feature to split the inputs.\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            candidate_thresholds = np.unique(X_column)\n",
        "            for threshold in candidate_thresholds:\n",
        "                gain = self._information_gain(y, X_column, threshold)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = threshold\n",
        "\n",
        "        return split_idx, split_threshold"
      ],
      "metadata": {
        "id": "2DSVjsMg37ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Measuring the best split\n",
        "\n",
        "There are different methods for quantifying information gain, and these differ between classification and regression trees. I will focus on Gini and Entropy for classification, but a regression tree may use Variance or Mean Squared Error reduction.\n",
        "\n",
        "For entropy, the proportion of samples in that class $P(c)$ is multiplied by the log base 2 of the class $log_2 p(c)$. This is done separtely for classes (above and below the threshold) and summed.\n",
        "\n",
        "$Entropy(feature) = - \\displaystyle\\sum_{c \\in \\text{classes}} p(c) \\log_2 p(c)$\n",
        "\n",
        "Gini impurity measures how often a randomly chosen data point would be misclassified if randomly labeled following the class distribution. In the equation we sum the squared probability $p^2$ of a data point belonging to a class and minus that sum from 1.\n",
        "\n",
        "$Gini\\ impurity = 1  -\\displaystyle\\sum_{c \\in \\text{classes}} p(c) \\log_2 p(c)$\n",
        "\n",
        "The methods described above measure the impurity of the split. To get the information gain we measure the difference in this impurity before (in the parent node) and after (in the child nodes) the split.\n",
        "\n",
        "`_information_gain` does this:\n",
        "* compute the impurity of parent node `parent_entropy`.\n",
        "* splt the node based on the feature and threshold provided by the `_best_split` for loop.\n",
        "* compute the impurity of the child nodes before averaging by weight and then summing together.\n",
        "*calculate the information gain = parent entropy - children entropy.\n",
        "\n",
        "      def _information_gain(self, y, X_column, split_threshold):\n",
        "\n",
        "        parent_entropy = information_measure(y, self.measure_name)\n",
        "\n",
        "        # generate split\n",
        "        left_idxs, right_idxs = self._split(X_column, split_threshold)\n",
        "\n",
        "        # if all elements fall in one side or the other the split has no effect\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "          return 0\n",
        "\n",
        "        # compute the weighted average of the entropy for the children\n",
        "        n = len(y)\n",
        "        left_child_entropy = information_measure(y[left_idxs],self.measure_name)\n",
        "        right_child_entropy = information_measure(y[right_idxs],self.measure_name)\n",
        "        left_weight = len(left_idxs) / n\n",
        "        right_weight = len(right_idxs) / n\n",
        "        children_entropy =  left_weight * left_child_entropy + right_weight * right_child_entropy\n",
        "\n",
        "        # information gain as the difference in entropy before and after split\n",
        "        information_gain = parent_entropy - children_entropy\n",
        "          return information_gain\n",
        "\n",
        "      def _split(self, X_column, split_threshold):\n",
        "          # returns 2 arrays, one with values <= threshold and the other > thershold\n",
        "          left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "          right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "          return left_idxs, right_idxs"
      ],
      "metadata": {
        "id": "tfNwU79FAF_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Predict\n",
        "\n",
        "After fitting the best splits to our data through training, we now want to predict unseen data. `_traverse_tree` does this by starting with the tree root, and calling the corresponding nodes of the parents left/right split. If the node has a value it is a leaf node and returns the result, otherwise it continues.\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        '''\n",
        "        Recursively traverse the tree.\n",
        "\n",
        "        Arguments:\n",
        "        x is the input vector that needs to be classified.\n",
        "        node is the current node.\n",
        "        '''\n",
        "        # if it is a leaf node returns its value\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n",
        "\n"
      ],
      "metadata": {
        "id": "f4xIhX5dG4R6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementation"
      ],
      "metadata": {
        "id": "d1JL1I7BJ15y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dnBLjfyYNSEn"
      },
      "outputs": [],
      "source": [
        "# Implementation\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "#Impurity calculations\n",
        "def entropy(y):\n",
        "    counts = np.bincount(y)\n",
        "    percentages  = counts / len(y)\n",
        "    entropy = 0\n",
        "\n",
        "    for p in percentages:\n",
        "      if p > 0:\n",
        "        entropy += p*np.log2(p)\n",
        "    return -entropy\n",
        "\n",
        "def gini_impurity(y):\n",
        "    counts = np.bincount(y)\n",
        "    probabilities = counts / len(y)\n",
        "    impurity = 1 - np.sum(probabilities**2)\n",
        "    return impurity\n",
        "\n",
        "def information_measure(y, name):\n",
        "    if name == \"entropy\":\n",
        "        return entropy(y)\n",
        "    elif name == \"gini\":\n",
        "        return gini_impurity(y)\n",
        "    else:\n",
        "        print(\"invalid measure name\")\n",
        "\n",
        "#Store node information\n",
        "class Node:\n",
        "    '''\n",
        "    Helper class holding node information.\n",
        "\n",
        "    Arguments:\n",
        "    feature -- which column the threshold will refer to\n",
        "    threshold -- the splitting parameter\n",
        "    left -- left child node\n",
        "    right -- right child node\n",
        "    value -- label of the node, exist only if it is leaf node.\n",
        "    '''\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "class DecisionTree:\n",
        "\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, measure_name=\"entropy\", n_feats=None):\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.n_feats = n_feats\n",
        "        self.root = None\n",
        "        self.measure_name = measure_name\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # if not specified the tree will use all the features\n",
        "        if self.n_feats != None:\n",
        "          self.n_feats = min(self.n_feats, X.shape[1])\n",
        "        else:\n",
        "          self.n_feats = X.shape[1]\n",
        "\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        # stopping criteria\n",
        "        if depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split:\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # pick a random subset of the features if n_feats is smaller than n_features (used by Random Forests)\n",
        "        feat_idxs = np.random.choice(n_features, self.n_feats, replace=False) # random forest\n",
        "\n",
        "        # greedily select the best split according to information gain\n",
        "        best_feat, best_thresh = self._best_split(X, y, feat_idxs)\n",
        "\n",
        "        # grow the children that result from the split\n",
        "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
        "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feat, best_thresh, left, right)\n",
        "\n",
        "    def _best_split(self, X, y, feat_idxs):\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "        #Find best feature and threshold combination\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            candidate_thresholds = np.unique(X_column)\n",
        "            for threshold in candidate_thresholds:\n",
        "                gain = self._information_gain(y, X_column, threshold)\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = threshold\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _information_gain(self, y, X_column, split_threshold):\n",
        "\n",
        "        parent_entropy = information_measure(y, self.measure_name)\n",
        "\n",
        "        # generate split\n",
        "        left_idxs, right_idxs = self._split(X_column, split_threshold)\n",
        "\n",
        "        # if all elements fall in one side or the other the split has no effect\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "          return 0\n",
        "\n",
        "        # compute the weighted average of the entropy for the children\n",
        "        n = len(y)\n",
        "        left_child_entropy = information_measure(y[left_idxs],self.measure_name)\n",
        "        right_child_entropy = information_measure(y[right_idxs],self.measure_name)\n",
        "        left_weight = len(left_idxs) / n\n",
        "        right_weight = len(right_idxs) / n\n",
        "        children_entropy =  left_weight * left_child_entropy + right_weight * right_child_entropy\n",
        "\n",
        "        # information gain as the difference in entropy before and after split\n",
        "        information_gain = parent_entropy - children_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_threshold):\n",
        "        # returns 2 arrays, one with all the indices of the values smaller or equal than the threshold and one with all the indices that are greater\n",
        "        left_idxs = np.argwhere(X_column <= split_threshold).flatten()\n",
        "        right_idxs = np.argwhere(X_column > split_threshold).flatten()\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        '''\n",
        "        Recursively traverse the tree.\n",
        "\n",
        "        Arguments:\n",
        "        x is the input vector that needs to be classified.\n",
        "        node is the current node.\n",
        "        '''\n",
        "        # if it is a leaf node returns its value\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MSNyvF5JNSEp",
        "outputId": "bc265009-1b26-4d2f-80f6-e0ae0e7357e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing time (s): 1.416699012\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.90      0.92        80\n",
            "           1       0.95      0.97      0.96       148\n",
            "\n",
            "    accuracy                           0.95       228\n",
            "   macro avg       0.95      0.94      0.94       228\n",
            "weighted avg       0.95      0.95      0.95       228\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import data\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "start = time.time_ns()\n",
        "# Default uses entropy for impurity measure\n",
        "tree = DecisionTree(max_depth=12)\n",
        "tree.fit(X_train, y_train)\n",
        "end = time.time_ns() - start\n",
        "# Computation duration (seconds)\n",
        "print(f\"Processing time (s): {end/1000000000}\")\n",
        "\n",
        "# Precision = True Positives /(True Positives + False Positives). i.e if I say class 0 what is the probability I am correct\n",
        "# Recall = TP / (TP + FN). i.e what proportion of class 0 did I predict correctly\n",
        "# f1-score = 2 * ((precision * recall)/(precision + recall))\n",
        "# Support = Number of true samples in that class\n",
        "\n",
        "y_pred = tree.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared with sklearn imported model."
      ],
      "metadata": {
        "id": "zvpZN6bn2fam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "sklearn_tree = DecisionTreeClassifier(max_depth=12)\n",
        "sklearn_tree.fit(X_train, y_train)\n",
        "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y3etfDd2H8t",
        "outputId": "f64dfb7e-261e-477b-a06d-06082f60938e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.90      0.92        80\n",
            "           1       0.95      0.97      0.96       148\n",
            "\n",
            "    accuracy                           0.95       228\n",
            "   macro avg       0.95      0.94      0.94       228\n",
            "weighted avg       0.95      0.95      0.95       228\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest\n"
      ],
      "metadata": {
        "id": "nL1PdhgNfpXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expantion to a Randon Forest is simple. It is an ensemble of decision trees with predictions being made on a majority vode of these trees/\n",
        "\n",
        "    for tree in forest:\n",
        "        randomly sample a portion of data for tree subset\n",
        "        call decision tree on that subset\n",
        "        append tree to self.trees\n",
        "    Count votes and return max\n",
        "\n",
        "In my code a prediction is made by majority vote of the individual tree outputs. However, other methods to do this are weighted averages based on training performance, or averaging probabilities instead of hard label predictions."
      ],
      "metadata": {
        "id": "23EJ-uY6ft1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:\n",
        "    def __init__(self, n_trees, portion=1, min_samples_split=2, max_depth=100,n_feats=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.n_feats = n_feats        # Number of features to condiser at each split\n",
        "        self.portion = portion        # Portion of data to put into a subset\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_trees):\n",
        "            tree = DecisionTree(\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                max_depth=self.max_depth,\n",
        "                n_feats=self.n_feats,\n",
        "            )\n",
        "            #bootstrapping\n",
        "            n_samples = X.shape[0]\n",
        "            subset_size = int(self.portion * n_samples)\n",
        "            #there are different ways to sample rather than random\n",
        "            indicies = np.random.choice(n_samples, subset_size, replace=True)\n",
        "            Xs = X[indicies]\n",
        "            ys = y[indicies]\n",
        "\n",
        "            #train\n",
        "            tree.fit(Xs, ys)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        votes = []\n",
        "        y_pred = []\n",
        "\n",
        "        for tree in self.trees:\n",
        "            votes.append(tree.predict(X))\n",
        "\n",
        "        votes = np.swapaxes(np.array(votes), 0, 1)\n",
        "        for v in votes:\n",
        "            values, counts = np.unique(v, return_counts=True)\n",
        "            index = np.argmax(counts)\n",
        "            y_pred.append(values[index])\n",
        "        return np.array(y_pred)"
      ],
      "metadata": {
        "id": "BK3eP7mggHyb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF = RandomForest(n_trees = 5, portion = 0.5, max_depth = 12, n_feats = 20)\n",
        "\n",
        "RF.fit(X_train, y_train)\n",
        "y_pred = RF.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "l5H8ZTLpgGhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb1128c-ffa6-460b-e211-c1221dfef659"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.94        80\n",
            "           1       0.97      0.97      0.97       148\n",
            "\n",
            "    accuracy                           0.96       228\n",
            "   macro avg       0.96      0.96      0.96       228\n",
            "weighted avg       0.96      0.96      0.96       228\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test against sklearn import\n",
        "# random sampling causes results to differ when re-running\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "sklearn_RF = RandomForestClassifier(n_estimators=5, max_depth=12)\n",
        "sklearn_RF.fit(X_train, y_train)\n",
        "y_pred_sklearn = sklearn_RF.predict(X_test)\n",
        "print(classification_report(y_test, y_pred_sklearn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z441ylNzsncV",
        "outputId": "50166325-faeb-40b8-fe22-93187da79bfe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.95      0.96        80\n",
            "           1       0.97      0.98      0.98       148\n",
            "\n",
            "    accuracy                           0.97       228\n",
            "   macro avg       0.97      0.96      0.97       228\n",
            "weighted avg       0.97      0.97      0.97       228\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "rise": {
      "enable_chalkboard": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}